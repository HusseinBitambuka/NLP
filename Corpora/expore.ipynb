{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import webtext\n",
    "from nltk.corpus import nps_chat\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Corpus and Corpora**\n",
    "\n",
    "## **Corpus** refers to one collection of texts.\n",
    "\n",
    "## **Corpora** refers to multiple collections of texts\n",
    "\n",
    "## **popular Corpora**\n",
    "\n",
    "- **The Brown Corpus**: Created by Brown University in 1961, this was the first million-word electronic corpus of English.\n",
    "\n",
    "- **The Gutenberg Corpus**: Containing 25,000 free electronic books, this data was taken from the Project Gutenberg electronic text archive of literature.\n",
    "\n",
    "- **The Web Text Corpus**: Containing less formal language, this data includes content from online discussion forums, personal advertisements, and reviews.\n",
    "\n",
    "- **The NPS Chat Corpus**: Originally created by the Naval Postgraduate School, this corpus contains over 10,000 posts from instant messaging chats.\n",
    "\n",
    "- **The Reuters Corpus**: This corpus contains over 10,000 news documents, grouped into two sets called 'training' and 'test'.\n",
    "\n",
    "- **The Inaugural Address Corpus**: This corpus contains each presidential inaugural address.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " inaugural corpus is broken up by word: ['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', ...]\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n inaugural corpus is broken up by word: {inaugural.words()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " using sents(): [['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House', 'of', 'Representatives', ':'], ['Among', 'the', 'vicissitudes', 'incident', 'to', 'life', 'no', 'event', 'could', 'have', 'filled', 'me', 'with', 'greater', 'anxieties', 'than', 'that', 'of', 'which', 'the', 'notification', 'was', 'transmitted', 'by', 'your', 'order', ',', 'and', 'received', 'on', 'the', '14th', 'day', 'of', 'the', 'present', 'month', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n using sents(): {inaugural.sents()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " using sents(): [[['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House', 'of', 'Representatives', ':']], [['Among', 'the', 'vicissitudes', 'incident', 'to', 'life', 'no', 'event', 'could', 'have', 'filled', 'me', 'with', 'greater', 'anxieties', 'than', 'that', 'of', 'which', 'the', 'notification', 'was', 'transmitted', 'by', 'your', 'order', ',', 'and', 'received', 'on', 'the', '14th', 'day', 'of', 'the', 'present', 'month', '.'], ['On', 'the', 'one', 'hand', ',', 'I', 'was', 'summoned', 'by', 'my', 'Country', ',', 'whose', 'voice', 'I', 'can', 'never', 'hear', 'but', 'with', 'veneration', 'and', 'love', ',', 'from', 'a', 'retreat', 'which', 'I', 'had', 'chosen', 'with', 'the', 'fondest', 'predilection', ',', 'and', ',', 'in', 'my', 'flattering', 'hopes', ',', 'with', 'an', 'immutable', 'decision', ',', 'as', 'the', 'asylum', 'of', 'my', 'declining', 'years', '--', 'a', 'retreat', 'which', 'was', 'rendered', 'every', 'day', 'more', 'necessary', 'as', 'well', 'as', 'more', 'dear', 'to', 'me', 'by', 'the', 'addition', 'of', 'habit', 'to', 'inclination', ',', 'and', 'of', 'frequent', 'interruptions', 'in', 'my', 'health', 'to', 'the', 'gradual', 'waste', 'committed', 'on', 'it', 'by', 'time', '.'], ['On', 'the', 'other', 'hand', ',', 'the', 'magnitude', 'and', 'difficulty', 'of', 'the', 'trust', 'to', 'which', 'the', 'voice', 'of', 'my', 'country', 'called', 'me', ',', 'being', 'sufficient', 'to', 'awaken', 'in', 'the', 'wisest', 'and', 'most', 'experienced', 'of', 'her', 'citizens', 'a', 'distrustful', 'scrutiny', 'into', 'his', 'qualifications', ',', 'could', 'not', 'but', 'overwhelm', 'with', 'despondence', 'one', 'who', '(', 'inheriting', 'inferior', 'endowments', 'from', 'nature', 'and', 'unpracticed', 'in', 'the', 'duties', 'of', 'civil', 'administration', ')', 'ought', 'to', 'be', 'peculiarly', 'conscious', 'of', 'his', 'own', 'deficiencies', '.'], ['In', 'this', 'conflict', 'of', 'emotions', 'all', 'I', 'dare', 'aver', 'is', 'that', 'it', 'has', 'been', 'my', 'faithful', 'study', 'to', 'collect', 'my', 'duty', 'from', 'a', 'just', 'appreciation', 'of', 'every', 'circumstance', 'by', 'which', 'it', 'might', 'be', 'affected', '.'], ['All', 'I', 'dare', 'hope', 'is', 'that', 'if', ',', 'in', 'executing', 'this', 'task', ',', 'I', 'have', 'been', 'too', 'much', 'swayed', 'by', 'a', 'grateful', 'remembrance', 'of', 'former', 'instances', ',', 'or', 'by', 'an', 'affectionate', 'sensibility', 'to', 'this', 'transcendent', 'proof', 'of', 'the', 'confidence', 'of', 'my', 'fellow', 'citizens', ',', 'and', 'have', 'thence', 'too', 'little', 'consulted', 'my', 'incapacity', 'as', 'well', 'as', 'disinclination', 'for', 'the', 'weighty', 'and', 'untried', 'cares', 'before', 'me', ',', 'my', 'error', 'will', 'be', 'palliated', 'by', 'the', 'motives', 'which', 'mislead', 'me', ',', 'and', 'its', 'consequences', 'be', 'judged', 'by', 'my', 'country', 'with', 'some', 'share', 'of', 'the', 'partiality', 'in', 'which', 'they', 'originated', '.']], ...]\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n using sents(): {inaugural.paras()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown tagged-words(): [('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n"
     ]
    }
   ],
   "source": [
    "print(f'Brown tagged-words(): {brown.tagged_words()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon Definition\n",
    "\n",
    "**Lexicon**: Often referred to as a lexical resource, a lexicon is a collection of words and/or phrases, such as a set of vocabulary or a dictionary, marked with allied information such as each given wordâ€™s part of speech or definition. A lexicon is often considered a type of corpus because it represents text data.\n",
    "\n",
    "## Accessing Lexicons in NLTK\n",
    "\n",
    "Several lexicons are included in the NLTK library. Some of the commonly used lexicons are:\n",
    "\n",
    "### The Stopwords Corpus\n",
    "\n",
    "Words like \"me\", \"has\", \"also\", and \"to\" are all examples of stop words. Stop words add little meaning to text data. This lexicon corpus contains stop words in English.\n",
    "\n",
    "### The Names Corpus\n",
    "\n",
    "Categorized by gender, this lexicon corpus contains over 8,000 first names. Female names are stored in the file called `female.txt` and male names in the file called `male.txt`.\n",
    "\n",
    "### The CMU Pronouncing Dictionary Corpus\n",
    "\n",
    "Based on US English, this lexicon corpus contains the phonetic pronunciations of words. Each phonetic pronunciation is represented with a symbol based on the Arpabet.\n",
    "\n",
    "## Access Code for Lexicons\n",
    "\n",
    "To access each of the above-mentioned lexicons, enter the code below:\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import names\n",
    "from nltk.corpus import cmudict\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import names\n",
    "from nltk.corpus import cmudict\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.45290941811638\n"
     ]
    }
   ],
   "source": [
    "female_names = names.words('female.txt')\n",
    "female_names_endingwith_a = [element for element in female_names if element.endswith('a')]\n",
    "female_percentage = 100* len(female_names_endingwith_a)/ len(female_names)\n",
    "print(female_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  artificial  deep  great  includes  intelligence  is  learning  \\\n",
      "0    0           0     0      1         0             0   1         1   \n",
      "1    0           0     1      0         0             0   1         2   \n",
      "2    1           1     1      0         1             1   0         2   \n",
      "\n",
      "   machine  of  subset  \n",
      "0        1   0       0  \n",
      "1        1   1       1  \n",
      "2        1   0       0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Machine learning is great.\",\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"Artificial intelligence includes machine learning and deep learning.\"\n",
    "]\n",
    "\n",
    "# Create the TDM\n",
    "vectorizer = CountVectorizer()\n",
    "tdm = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to DataFrame\n",
    "tdm_df = pd.DataFrame(tdm.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(tdm_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        and  artificial      deep    great  includes  intelligence        is  \\\n",
      "0  0.000000    0.000000  0.000000  0.66284  0.000000      0.000000  0.504107   \n",
      "1  0.000000    0.000000  0.343538  0.00000  0.000000      0.000000  0.343538   \n",
      "2  0.397699    0.397699  0.302460  0.00000  0.397699      0.397699  0.000000   \n",
      "\n",
      "   learning   machine        of    subset  \n",
      "0  0.391484  0.391484  0.000000  0.000000  \n",
      "1  0.533575  0.266788  0.451711  0.451711  \n",
      "2  0.469775  0.234887  0.000000  0.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create the TF-IDF matrix\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(tfidf_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
