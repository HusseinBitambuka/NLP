{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests as rqst\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn',\n",
       " 'abigail',\n",
       " 'emily',\n",
       " 'elizabeth',\n",
       " 'mila',\n",
       " 'ella',\n",
       " 'avery',\n",
       " 'sofia',\n",
       " 'camila',\n",
       " 'aria',\n",
       " 'scarlett']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(\"\".join(words))))\n",
    "print(len(chars))\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '.': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(\"\".join(words))))\n",
    "print(len(chars))\n",
    "char_to_i = {char:i + 1 for i, char in enumerate(chars)} # look-up table for mapping char to its respective index\n",
    "i_to_char = {i + 1:char for i, char in enumerate(chars)} # look-up table for mapping index to its respective char\n",
    "char_to_i['.'] = 0\n",
    "i_to_char[0] = '.'\n",
    "char_to_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "char1: . and char2: e\n",
      "char1: e and char2: m\n",
      "char1: m and char2: m\n",
      "char1: m and char2: a\n",
      "char1: a and char2: .\n"
     ]
    }
   ],
   "source": [
    "# create a trainign set\n",
    "xs, ys = [], []\n",
    "for w in words[:1]:\n",
    "    print(words[0])\n",
    "    chrs = ['.'] + list (w) + ['.']\n",
    "    for ch1, ch2 in zip(chrs, chrs[1:]):\n",
    "        print(f\"char1: {ch1} and char2: {ch2}\")\n",
    "        ix1 = char_to_i[ch1]\n",
    "        ix2 = char_to_i[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at the input and output. what we want is each time the input from the index to get the output from the a similar index. ex: if we give xs[0], we should get ys[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The issue with that approach is that we are going to use gradient descent, and it is not going to work well. so we are going to use one-hot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27)\n",
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x70b67c2c6840>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Always check the data type. we want the input to be floating points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build our first Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1807],\n",
       "        [-1.0667],\n",
       "        [ 1.9236],\n",
       "        [ 1.9236],\n",
       "        [ 1.3128]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn(27, 1) # our first single neuron\n",
    "\n",
    "xenc @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4116, -1.2639, -0.9761, -1.3654,  0.4495,  0.0230,  1.3763,  0.6261,\n",
       "          0.9768,  0.3988,  0.8456, -2.0865, -0.4426, -0.0106,  0.3013, -0.6279,\n",
       "         -1.3391,  1.3274, -1.3177, -0.4362, -0.9034,  0.2959, -0.8192, -0.9006,\n",
       "         -0.0351,  0.5901,  0.7649],\n",
       "        [ 0.3517, -0.4966, -0.1916, -0.2971,  0.7546,  0.0347,  0.2870, -0.4850,\n",
       "          0.2197, -1.6525, -0.2674,  0.8202,  0.3084, -1.5518, -1.6056, -0.0839,\n",
       "         -0.9802,  0.0514,  2.2725,  0.7936, -0.8220,  1.1715,  1.3381, -0.1415,\n",
       "          0.7616,  0.9762,  0.6729],\n",
       "        [-0.4050, -0.7204, -1.3445, -0.3927,  1.0047, -0.6007, -2.2277, -0.2545,\n",
       "         -0.0435, -1.4564, -1.4929,  0.8781,  0.7642,  0.0564,  0.3448,  0.6803,\n",
       "          0.1918,  1.3059, -0.9183, -0.4906, -1.9876, -0.4174,  0.7898,  0.7817,\n",
       "         -0.0755, -0.1760,  0.9522],\n",
       "        [-0.4050, -0.7204, -1.3445, -0.3927,  1.0047, -0.6007, -2.2277, -0.2545,\n",
       "         -0.0435, -1.4564, -1.4929,  0.8781,  0.7642,  0.0564,  0.3448,  0.6803,\n",
       "          0.1918,  1.3059, -0.9183, -0.4906, -1.9876, -0.4174,  0.7898,  0.7817,\n",
       "         -0.0755, -0.1760,  0.9522],\n",
       "        [-0.4007, -1.0383,  0.0375,  1.0181, -0.0793,  0.3479, -2.2987,  0.4541,\n",
       "         -0.6133, -0.5061,  1.0893,  1.0586, -0.1339, -1.2714,  0.8330, -0.1523,\n",
       "          0.3286, -0.3269,  1.1611,  1.5752,  1.9284,  0.9164, -0.1216,  1.8165,\n",
       "         -0.1871,  0.8878,  0.1632]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn(27, 27) # now we want to do 27 neurons. this is one layer with 27 neurons\n",
    "xenc @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0034, 0.0015, 0.0020, 0.0013, 0.0082, 0.0053, 0.0206, 0.0097, 0.0138,\n",
       "         0.0077, 0.0121, 0.0006, 0.0033, 0.0051, 0.0070, 0.0028, 0.0014, 0.0196,\n",
       "         0.0014, 0.0034, 0.0021, 0.0070, 0.0023, 0.0021, 0.0050, 0.0094, 0.0112],\n",
       "        [0.0074, 0.0032, 0.0043, 0.0039, 0.0111, 0.0054, 0.0069, 0.0032, 0.0065,\n",
       "         0.0010, 0.0040, 0.0118, 0.0071, 0.0011, 0.0010, 0.0048, 0.0020, 0.0055,\n",
       "         0.0505, 0.0115, 0.0023, 0.0168, 0.0198, 0.0045, 0.0111, 0.0138, 0.0102],\n",
       "        [0.0035, 0.0025, 0.0014, 0.0035, 0.0142, 0.0029, 0.0006, 0.0040, 0.0050,\n",
       "         0.0012, 0.0012, 0.0125, 0.0112, 0.0055, 0.0073, 0.0103, 0.0063, 0.0192,\n",
       "         0.0021, 0.0032, 0.0007, 0.0034, 0.0115, 0.0114, 0.0048, 0.0044, 0.0135],\n",
       "        [0.0035, 0.0025, 0.0014, 0.0035, 0.0142, 0.0029, 0.0006, 0.0040, 0.0050,\n",
       "         0.0012, 0.0012, 0.0125, 0.0112, 0.0055, 0.0073, 0.0103, 0.0063, 0.0192,\n",
       "         0.0021, 0.0032, 0.0007, 0.0034, 0.0115, 0.0114, 0.0048, 0.0044, 0.0135],\n",
       "        [0.0035, 0.0018, 0.0054, 0.0144, 0.0048, 0.0074, 0.0005, 0.0082, 0.0028,\n",
       "         0.0031, 0.0155, 0.0150, 0.0045, 0.0015, 0.0120, 0.0045, 0.0072, 0.0038,\n",
       "         0.0166, 0.0251, 0.0358, 0.0130, 0.0046, 0.0320, 0.0043, 0.0126, 0.0061]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = (xenc @ W) # log-count\n",
    "counts = logits.exp() # equivalent of N in Bigrams\n",
    "probs = counts/ counts.sum()\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # using seed to ensure replicability\n",
    "W = torch.randn((27,27), generator=g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0195, 0.0032, 0.0039, 0.0014, 0.0054, 0.0039, 0.0009, 0.0074, 0.0044,\n",
       "         0.0100, 0.0025, 0.0089, 0.0029, 0.0026, 0.0160, 0.0763, 0.0194, 0.0008,\n",
       "         0.0080, 0.0018, 0.0109, 0.0035, 0.0009, 0.0064, 0.0038, 0.0493, 0.0468],\n",
       "        [0.0065, 0.0179, 0.0056, 0.0117, 0.0447, 0.0065, 0.0021, 0.0075, 0.0022,\n",
       "         0.0068, 0.0158, 0.0051, 0.0026, 0.0041, 0.0024, 0.0071, 0.0065, 0.0010,\n",
       "         0.0206, 0.0048, 0.0109, 0.0067, 0.0113, 0.0006, 0.0026, 0.0005, 0.0106],\n",
       "        [0.0049, 0.0116, 0.0077, 0.0053, 0.0106, 0.0032, 0.0042, 0.0039, 0.0194,\n",
       "         0.0026, 0.0012, 0.0125, 0.0021, 0.0042, 0.0023, 0.0018, 0.0092, 0.0019,\n",
       "         0.0103, 0.0009, 0.0033, 0.0012, 0.0021, 0.0032, 0.0190, 0.0074, 0.0020],\n",
       "        [0.0049, 0.0116, 0.0077, 0.0053, 0.0106, 0.0032, 0.0042, 0.0039, 0.0194,\n",
       "         0.0026, 0.0012, 0.0125, 0.0021, 0.0042, 0.0023, 0.0018, 0.0092, 0.0019,\n",
       "         0.0103, 0.0009, 0.0033, 0.0012, 0.0021, 0.0032, 0.0190, 0.0074, 0.0020],\n",
       "        [0.0021, 0.0012, 0.0055, 0.0014, 0.0084, 0.0043, 0.0150, 0.0018, 0.0017,\n",
       "         0.0007, 0.0142, 0.0012, 0.0137, 0.0015, 0.0032, 0.0029, 0.0057, 0.0011,\n",
       "         0.0125, 0.0074, 0.0064, 0.0043, 0.0007, 0.0046, 0.0091, 0.0070, 0.0013]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "logits = (xenc @ W) # log-count\n",
    "counts = logits.exp() # equivalent of N in Bigrams\n",
    "probs = counts/ counts.sum() # probability for the next character ( the two consecutive lines are sofmax)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "\n",
      " bigram example 1:. e (indeces 0, 5)\n",
      "\n",
      " input to the neural network is: 0\n",
      "\n",
      " output probabilities from  the neural network is: tensor([0.0195, 0.0032, 0.0039, 0.0014, 0.0054, 0.0039, 0.0009, 0.0074, 0.0044,\n",
      "        0.0100, 0.0025, 0.0089, 0.0029, 0.0026, 0.0160, 0.0763, 0.0194, 0.0008,\n",
      "        0.0080, 0.0018, 0.0109, 0.0035, 0.0009, 0.0064, 0.0038, 0.0493, 0.0468])\n",
      "\n",
      " lebel ( actual next character) is : 5\n",
      "\n",
      " probability assigned to the correct next char by the net is: 0.003941063769161701\n",
      "\n",
      " log likelihood is: -5.536304473876953\n",
      "\n",
      " negative log likelihood: 5.536304473876953\n",
      "===========================\n",
      "\n",
      " bigram example 2:e m (indeces 5, 13)\n",
      "\n",
      " input to the neural network is: 5\n",
      "\n",
      " output probabilities from  the neural network is: tensor([0.0065, 0.0179, 0.0056, 0.0117, 0.0447, 0.0065, 0.0021, 0.0075, 0.0022,\n",
      "        0.0068, 0.0158, 0.0051, 0.0026, 0.0041, 0.0024, 0.0071, 0.0065, 0.0010,\n",
      "        0.0206, 0.0048, 0.0109, 0.0067, 0.0113, 0.0006, 0.0026, 0.0005, 0.0106])\n",
      "\n",
      " lebel ( actual next character) is : 13\n",
      "\n",
      " probability assigned to the correct next char by the net is: 0.0040543111972510815\n",
      "\n",
      " log likelihood is: -5.507974624633789\n",
      "\n",
      " negative log likelihood: 5.507974624633789\n",
      "===========================\n",
      "\n",
      " bigram example 3:m m (indeces 13, 13)\n",
      "\n",
      " input to the neural network is: 13\n",
      "\n",
      " output probabilities from  the neural network is: tensor([0.0049, 0.0116, 0.0077, 0.0053, 0.0106, 0.0032, 0.0042, 0.0039, 0.0194,\n",
      "        0.0026, 0.0012, 0.0125, 0.0021, 0.0042, 0.0023, 0.0018, 0.0092, 0.0019,\n",
      "        0.0103, 0.0009, 0.0033, 0.0012, 0.0021, 0.0032, 0.0190, 0.0074, 0.0020])\n",
      "\n",
      " lebel ( actual next character) is : 13\n",
      "\n",
      " probability assigned to the correct next char by the net is: 0.004216378089040518\n",
      "\n",
      " log likelihood is: -5.468778610229492\n",
      "\n",
      " negative log likelihood: 5.468778610229492\n",
      "===========================\n",
      "\n",
      " bigram example 4:m a (indeces 13, 1)\n",
      "\n",
      " input to the neural network is: 13\n",
      "\n",
      " output probabilities from  the neural network is: tensor([0.0049, 0.0116, 0.0077, 0.0053, 0.0106, 0.0032, 0.0042, 0.0039, 0.0194,\n",
      "        0.0026, 0.0012, 0.0125, 0.0021, 0.0042, 0.0023, 0.0018, 0.0092, 0.0019,\n",
      "        0.0103, 0.0009, 0.0033, 0.0012, 0.0021, 0.0032, 0.0190, 0.0074, 0.0020])\n",
      "\n",
      " lebel ( actual next character) is : 1\n",
      "\n",
      " probability assigned to the correct next char by the net is: 0.011638503521680832\n",
      "\n",
      " log likelihood is: -4.453436374664307\n",
      "\n",
      " negative log likelihood: 4.453436374664307\n",
      "===========================\n",
      "\n",
      " bigram example 5:a . (indeces 1, 0)\n",
      "\n",
      " input to the neural network is: 1\n",
      "\n",
      " output probabilities from  the neural network is: tensor([0.0021, 0.0012, 0.0055, 0.0014, 0.0084, 0.0043, 0.0150, 0.0018, 0.0017,\n",
      "        0.0007, 0.0142, 0.0012, 0.0137, 0.0015, 0.0032, 0.0029, 0.0057, 0.0011,\n",
      "        0.0125, 0.0074, 0.0064, 0.0043, 0.0007, 0.0046, 0.0091, 0.0070, 0.0013])\n",
      "\n",
      " lebel ( actual next character) is : 0\n",
      "\n",
      " probability assigned to the correct next char by the net is: 0.002077222103253007\n",
      "\n",
      " log likelihood is: -6.176723957061768\n",
      "\n",
      " negative log likelihood: 6.176723957061768\n",
      "====================================\n",
      "\n",
      " the avarage log likelihood, i.e loss = 3.392902135848999\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(8)\n",
    "for i in range(5):\n",
    "    #i-th bigram\n",
    "    x =xs[i].item()\n",
    "    y = ys[i].item()\n",
    "    print('===========================')\n",
    "    print(f\"\\n bigram example {i + 1 }:{i_to_char[x]} {i_to_char[y]} (indeces {x}, {y})\")\n",
    "    print(f\"\\n input to the neural network is: {x}\")\n",
    "    print(f\"\\n output probabilities from  the neural network is: {probs[i]}\")\n",
    "    print(f\"\\n lebel ( actual next character) is : {y}\")\n",
    "    p = probs[i,y]\n",
    "    print(f\"\\n probability assigned to the correct next char by the net is: {p.item()}\")\n",
    "    logp = torch.log(p)\n",
    "    print(f\"\\n log likelihood is: {logp.item()}\")\n",
    "    nll = -logp\n",
    "    print(f\"\\n negative log likelihood: {nll.item()}\")\n",
    "    nlls[i] = nll\n",
    "print(\"====================================\")\n",
    "print(f\"\\n the avarage log likelihood, i.e loss = {nlls.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # using seed to ensure replicability\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0195, 0.0032, 0.0040, 0.0014, 0.0054, 0.0040, 0.0009, 0.0074, 0.0044,\n",
       "         0.0100, 0.0025, 0.0089, 0.0029, 0.0026, 0.0160, 0.0758, 0.0193, 0.0008,\n",
       "         0.0080, 0.0018, 0.0109, 0.0035, 0.0009, 0.0064, 0.0038, 0.0491, 0.0467],\n",
       "        [0.0065, 0.0179, 0.0056, 0.0117, 0.0445, 0.0065, 0.0021, 0.0075, 0.0022,\n",
       "         0.0068, 0.0158, 0.0051, 0.0026, 0.0041, 0.0024, 0.0071, 0.0065, 0.0010,\n",
       "         0.0206, 0.0048, 0.0109, 0.0067, 0.0113, 0.0006, 0.0027, 0.0005, 0.0106],\n",
       "        [0.0049, 0.0119, 0.0077, 0.0053, 0.0106, 0.0032, 0.0042, 0.0039, 0.0193,\n",
       "         0.0026, 0.0012, 0.0125, 0.0021, 0.0043, 0.0023, 0.0018, 0.0092, 0.0019,\n",
       "         0.0103, 0.0009, 0.0033, 0.0012, 0.0021, 0.0032, 0.0190, 0.0074, 0.0020],\n",
       "        [0.0049, 0.0119, 0.0077, 0.0053, 0.0106, 0.0032, 0.0042, 0.0039, 0.0193,\n",
       "         0.0026, 0.0012, 0.0125, 0.0021, 0.0043, 0.0023, 0.0018, 0.0092, 0.0019,\n",
       "         0.0103, 0.0009, 0.0033, 0.0012, 0.0021, 0.0032, 0.0190, 0.0074, 0.0020],\n",
       "        [0.0021, 0.0012, 0.0055, 0.0014, 0.0084, 0.0043, 0.0150, 0.0018, 0.0017,\n",
       "         0.0007, 0.0142, 0.0012, 0.0137, 0.0015, 0.0032, 0.0029, 0.0057, 0.0011,\n",
       "         0.0125, 0.0074, 0.0064, 0.0043, 0.0007, 0.0046, 0.0091, 0.0070, 0.0013]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "logits = (xenc @ W) # log-count\n",
    "counts = logits.exp() # equivalent of N in Bigrams\n",
    "probs = counts/ counts.sum() # probability for the next character ( the two consecutive lines are sofmax)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0039, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0041, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0042, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0116, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0021, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since it is one-hot encoded, we are interested in probabilities at the given index\n",
    "\n",
    "probs[0,5], probs[1, 13], probs[2, 13], probs[3, 1], probs[4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0039, 0.0041, 0.0042, 0.0116, 0.0021], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the probabilites with pytorch using the torch.arage fonction\n",
    "\n",
    "print(torch.arange(5))\n",
    "\n",
    "probs[torch.arange(5), ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4080, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -probs[torch.arange(5), ys].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.grad=None\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " shape is torch.Size([27, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0195,  0.0032,  0.0039,  0.0014,  0.0054, -0.1961,  0.0009,  0.0074,\n",
       "          0.0044,  0.0100,  0.0025,  0.0089,  0.0029,  0.0026,  0.0160,  0.0763,\n",
       "          0.0194,  0.0008,  0.0080,  0.0018,  0.0109,  0.0035,  0.0009,  0.0064,\n",
       "          0.0038,  0.0493,  0.0468],\n",
       "        [-0.1979,  0.0012,  0.0055,  0.0014,  0.0084,  0.0043,  0.0150,  0.0018,\n",
       "          0.0017,  0.0007,  0.0142,  0.0012,  0.0137,  0.0015,  0.0032,  0.0029,\n",
       "          0.0057,  0.0011,  0.0125,  0.0074,  0.0064,  0.0043,  0.0007,  0.0046,\n",
       "          0.0091,  0.0070,  0.0013],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0065,  0.0179,  0.0056,  0.0117,  0.0447,  0.0065,  0.0021,  0.0075,\n",
       "          0.0022,  0.0068,  0.0158,  0.0051,  0.0026, -0.1959,  0.0024,  0.0071,\n",
       "          0.0065,  0.0010,  0.0206,  0.0048,  0.0109,  0.0067,  0.0113,  0.0006,\n",
       "          0.0026,  0.0005,  0.0106],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0099, -0.1767,  0.0153,  0.0105,  0.0213,  0.0063,  0.0083,  0.0079,\n",
       "          0.0387,  0.0052,  0.0024,  0.0249,  0.0041, -0.1916,  0.0046,  0.0036,\n",
       "          0.0185,  0.0038,  0.0205,  0.0018,  0.0066,  0.0025,  0.0042,  0.0064,\n",
       "          0.0381,  0.0148,  0.0040],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\" shape is {W.grad.shape}\")\n",
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update the tensor\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 228146\n"
     ]
    }
   ],
   "source": [
    "# create a trainign set\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chrs = ['.'] + list (w) + ['.']\n",
    "    for ch1, ch2 in zip(chrs, chrs[1:]):\n",
    "        #print(f\"char1: {ch1} and char2: {ch2}\")\n",
    "        ix1 = char_to_i[ch1]\n",
    "        ix2 = char_to_i[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print(f\"number of examples: {num}\")\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.758953332901001\n",
      "3.3710989952087402\n",
      "3.1540420055389404\n",
      "3.020373582839966\n",
      "2.9277119636535645\n",
      "2.8604023456573486\n",
      "2.8097293376922607\n",
      "2.7701027393341064\n",
      "2.7380733489990234\n",
      "2.711496591567993\n",
      "2.6890034675598145\n",
      "2.6696884632110596\n",
      "2.6529300212860107\n",
      "2.638277292251587\n",
      "2.6253881454467773\n",
      "2.613990545272827\n",
      "2.603863477706909\n",
      "2.5948219299316406\n",
      "2.5867116451263428\n",
      "2.57940411567688\n",
      "2.572789192199707\n",
      "2.5667762756347656\n",
      "2.5612878799438477\n",
      "2.5562589168548584\n",
      "2.551633596420288\n",
      "2.547366142272949\n",
      "2.5434155464172363\n",
      "2.5397486686706543\n",
      "2.536336660385132\n",
      "2.533154249191284\n",
      "2.5301806926727295\n",
      "2.5273969173431396\n",
      "2.5247864723205566\n",
      "2.522334575653076\n",
      "2.520029067993164\n",
      "2.517857789993286\n",
      "2.515810966491699\n",
      "2.513878345489502\n",
      "2.512052059173584\n",
      "2.510324001312256\n",
      "2.5086867809295654\n",
      "2.5071346759796143\n",
      "2.5056614875793457\n",
      "2.5042612552642822\n",
      "2.502929210662842\n",
      "2.5016613006591797\n",
      "2.5004520416259766\n",
      "2.4992990493774414\n",
      "2.498197317123413\n",
      "2.497144937515259\n",
      "2.496137857437134\n",
      "2.495173692703247\n",
      "2.4942495822906494\n",
      "2.49336314201355\n",
      "2.4925124645233154\n",
      "2.4916954040527344\n",
      "2.4909095764160156\n",
      "2.4901540279388428\n",
      "2.4894261360168457\n",
      "2.488725185394287\n",
      "2.488049268722534\n",
      "2.4873976707458496\n",
      "2.4867680072784424\n",
      "2.4861602783203125\n",
      "2.4855728149414062\n",
      "2.4850049018859863\n",
      "2.484455108642578\n",
      "2.4839234352111816\n",
      "2.483407735824585\n",
      "2.4829084873199463\n",
      "2.482424259185791\n",
      "2.481955051422119\n",
      "2.481499195098877\n",
      "2.4810571670532227\n",
      "2.4806275367736816\n",
      "2.480210065841675\n",
      "2.479804515838623\n",
      "2.479410409927368\n",
      "2.4790267944335938\n",
      "2.4786536693573\n",
      "2.478290557861328\n",
      "2.4779367446899414\n",
      "2.477592706680298\n",
      "2.477257490158081\n",
      "2.4769303798675537\n",
      "2.476611852645874\n",
      "2.4763007164001465\n",
      "2.4759981632232666\n",
      "2.4757025241851807\n",
      "2.4754140377044678\n",
      "2.475132703781128\n",
      "2.4748575687408447\n",
      "2.4745898246765137\n",
      "2.474327802658081\n",
      "2.474071502685547\n",
      "2.4738216400146484\n",
      "2.4735772609710693\n",
      "2.4733383655548096\n",
      "2.473104476928711\n",
      "2.4728763103485107\n"
     ]
    }
   ],
   "source": [
    "# putting everything together now\n",
    "\n",
    "for k in range(100):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = (xenc @ W) # log-count\n",
    "    counts = logits.exp() # equivalent of N in Bigrams\n",
    "    probs = counts/ counts.sum(1, keepdim=True) # probability for the next character ( the two consecutive lines are sofmax)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    #backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    W.data += -50 * W.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
