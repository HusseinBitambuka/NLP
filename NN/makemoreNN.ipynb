{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests as rqst\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn',\n",
       " 'abigail',\n",
       " 'emily',\n",
       " 'elizabeth',\n",
       " 'mila',\n",
       " 'ella',\n",
       " 'avery',\n",
       " 'sofia',\n",
       " 'camila',\n",
       " 'aria',\n",
       " 'scarlett']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(\"\".join(words))))\n",
    "print(len(chars))\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '.': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(\"\".join(words))))\n",
    "print(len(chars))\n",
    "char_to_i = {char:i + 1 for i, char in enumerate(chars)} # look-up table for mapping char to its respective index\n",
    "i_to_char = {i + 1:char for i, char in enumerate(chars)} # look-up table for mapping index to its respective char\n",
    "char_to_i['.'] = 0\n",
    "i_to_char[0] = '.'\n",
    "char_to_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "char1: . and char2: e\n",
      "char1: e and char2: m\n",
      "char1: m and char2: m\n",
      "char1: m and char2: a\n",
      "char1: a and char2: .\n"
     ]
    }
   ],
   "source": [
    "# create a trainign set\n",
    "xs, ys = [], []\n",
    "for w in words[:1]:\n",
    "    print(words[0])\n",
    "    chrs = ['.'] + list (w) + ['.']\n",
    "    for ch1, ch2 in zip(chrs, chrs[1:]):\n",
    "        print(f\"char1: {ch1} and char2: {ch2}\")\n",
    "        ix1 = char_to_i[ch1]\n",
    "        ix2 = char_to_i[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at the input and output. what we want is each time the input from the index to get the output from the a similar index. ex: if we give xs[0], we should get ys[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The issue with that approach is that we are going to use gradient descent, and it is not going to work well. so we are going to use one-hot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27)\n",
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x724a218ab2f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Always check the data type. we want the input to be floating points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build our first Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1767],\n",
       "        [-1.1090],\n",
       "        [-1.7885],\n",
       "        [-1.7885],\n",
       "        [ 1.0068]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn(27, 1) # our first single neuron\n",
    "\n",
    "xenc @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3238, -1.2226, -0.9824,  0.5569,  0.7953, -0.6206,  2.1135, -0.1069,\n",
       "         -0.0091,  0.4126,  0.6255, -0.9672, -1.2209, -0.8899, -1.6296, -0.3130,\n",
       "         -0.8114,  0.6547,  0.6757,  1.2380, -0.6824, -1.5186, -1.1815,  3.1385,\n",
       "          0.6952, -1.0939, -0.6105],\n",
       "        [-0.6955, -0.0756, -0.6050,  1.4890, -0.7697,  1.6652,  0.2490,  0.3804,\n",
       "          0.1246, -0.9913,  0.0527,  0.3999,  0.3199, -0.8025,  1.1395, -0.6195,\n",
       "         -0.7696,  0.0069,  2.2055,  0.0436, -0.9435, -0.3426,  0.6122,  0.5062,\n",
       "         -0.4940, -0.0923,  0.4019],\n",
       "        [-0.3864,  1.6869, -1.8885,  1.3520,  1.0739, -1.0152,  0.0559,  0.6701,\n",
       "          0.6500, -1.0715, -0.7279, -0.5315,  0.7498,  1.1094,  0.2667, -0.0936,\n",
       "          0.7612,  2.3447,  0.9666,  1.1950, -0.5455,  1.0239,  0.3273,  1.3278,\n",
       "          0.2568,  0.2832, -1.1392],\n",
       "        [-0.3864,  1.6869, -1.8885,  1.3520,  1.0739, -1.0152,  0.0559,  0.6701,\n",
       "          0.6500, -1.0715, -0.7279, -0.5315,  0.7498,  1.1094,  0.2667, -0.0936,\n",
       "          0.7612,  2.3447,  0.9666,  1.1950, -0.5455,  1.0239,  0.3273,  1.3278,\n",
       "          0.2568,  0.2832, -1.1392],\n",
       "        [ 0.1808,  0.9835, -0.8759,  0.7336,  0.1236,  0.2702, -0.9506,  0.5007,\n",
       "         -0.7604, -2.5760, -1.3232,  1.2094,  1.5911,  0.6653, -0.8412,  1.2979,\n",
       "          0.1213,  0.8367,  0.1271, -0.8696, -1.9275,  1.7530, -1.3135, -0.3223,\n",
       "          1.0407,  0.2532, -0.6559]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn(27, 27) # now we want to do 27 neurons. this is one layer with 27 neurons\n",
    "xenc @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0145, 0.0011, 0.0014, 0.0067, 0.0085, 0.0021, 0.0319, 0.0035, 0.0038,\n",
       "         0.0058, 0.0072, 0.0015, 0.0011, 0.0016, 0.0008, 0.0028, 0.0017, 0.0074,\n",
       "         0.0076, 0.0133, 0.0019, 0.0008, 0.0012, 0.0889, 0.0077, 0.0013, 0.0021],\n",
       "        [0.0019, 0.0036, 0.0021, 0.0171, 0.0018, 0.0204, 0.0049, 0.0056, 0.0044,\n",
       "         0.0014, 0.0041, 0.0057, 0.0053, 0.0017, 0.0120, 0.0021, 0.0018, 0.0039,\n",
       "         0.0350, 0.0040, 0.0015, 0.0027, 0.0071, 0.0064, 0.0024, 0.0035, 0.0058],\n",
       "        [0.0026, 0.0208, 0.0006, 0.0149, 0.0113, 0.0014, 0.0041, 0.0075, 0.0074,\n",
       "         0.0013, 0.0019, 0.0023, 0.0082, 0.0117, 0.0050, 0.0035, 0.0082, 0.0402,\n",
       "         0.0101, 0.0127, 0.0022, 0.0107, 0.0053, 0.0145, 0.0050, 0.0051, 0.0012],\n",
       "        [0.0026, 0.0208, 0.0006, 0.0149, 0.0113, 0.0014, 0.0041, 0.0075, 0.0074,\n",
       "         0.0013, 0.0019, 0.0023, 0.0082, 0.0117, 0.0050, 0.0035, 0.0082, 0.0402,\n",
       "         0.0101, 0.0127, 0.0022, 0.0107, 0.0053, 0.0145, 0.0050, 0.0051, 0.0012],\n",
       "        [0.0046, 0.0103, 0.0016, 0.0080, 0.0044, 0.0050, 0.0015, 0.0064, 0.0018,\n",
       "         0.0003, 0.0010, 0.0129, 0.0189, 0.0075, 0.0017, 0.0141, 0.0044, 0.0089,\n",
       "         0.0044, 0.0016, 0.0006, 0.0222, 0.0010, 0.0028, 0.0109, 0.0050, 0.0020]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = (xenc @ W) # log-count\n",
    "counts = logits.exp() # equivalent of N in Bigrams\n",
    "probs = counts/ counts.sum()\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # using seed to ensure replicability\n",
    "W = torch.randn((27,27), generator=g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0195, 0.0032, 0.0039, 0.0014, 0.0054, 0.0039, 0.0009, 0.0074, 0.0044,\n",
       "         0.0100, 0.0025, 0.0089, 0.0029, 0.0026, 0.0160, 0.0763, 0.0194, 0.0008,\n",
       "         0.0080, 0.0018, 0.0109, 0.0035, 0.0009, 0.0064, 0.0038, 0.0493, 0.0468],\n",
       "        [0.0065, 0.0179, 0.0056, 0.0117, 0.0447, 0.0065, 0.0021, 0.0075, 0.0022,\n",
       "         0.0068, 0.0158, 0.0051, 0.0026, 0.0041, 0.0024, 0.0071, 0.0065, 0.0010,\n",
       "         0.0206, 0.0048, 0.0109, 0.0067, 0.0113, 0.0006, 0.0026, 0.0005, 0.0106],\n",
       "        [0.0049, 0.0116, 0.0077, 0.0053, 0.0106, 0.0032, 0.0042, 0.0039, 0.0194,\n",
       "         0.0026, 0.0012, 0.0125, 0.0021, 0.0042, 0.0023, 0.0018, 0.0092, 0.0019,\n",
       "         0.0103, 0.0009, 0.0033, 0.0012, 0.0021, 0.0032, 0.0190, 0.0074, 0.0020],\n",
       "        [0.0049, 0.0116, 0.0077, 0.0053, 0.0106, 0.0032, 0.0042, 0.0039, 0.0194,\n",
       "         0.0026, 0.0012, 0.0125, 0.0021, 0.0042, 0.0023, 0.0018, 0.0092, 0.0019,\n",
       "         0.0103, 0.0009, 0.0033, 0.0012, 0.0021, 0.0032, 0.0190, 0.0074, 0.0020],\n",
       "        [0.0021, 0.0012, 0.0055, 0.0014, 0.0084, 0.0043, 0.0150, 0.0018, 0.0017,\n",
       "         0.0007, 0.0142, 0.0012, 0.0137, 0.0015, 0.0032, 0.0029, 0.0057, 0.0011,\n",
       "         0.0125, 0.0074, 0.0064, 0.0043, 0.0007, 0.0046, 0.0091, 0.0070, 0.0013]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "logits = (xenc @ W) # log-count\n",
    "counts = logits.exp() # equivalent of N in Bigrams\n",
    "probs = counts/ counts.sum() # probability for the next character ( the two consecutive lines are sofmax)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "\n",
      " bigram example 1:. e (indeces 0, 5)\n",
      "\n",
      " input to the neural network is: 0\n",
      "\n",
      " output probabilities from  the neural network is: tensor([0.0195, 0.0032, 0.0039, 0.0014, 0.0054, 0.0039, 0.0009, 0.0074, 0.0044,\n",
      "        0.0100, 0.0025, 0.0089, 0.0029, 0.0026, 0.0160, 0.0763, 0.0194, 0.0008,\n",
      "        0.0080, 0.0018, 0.0109, 0.0035, 0.0009, 0.0064, 0.0038, 0.0493, 0.0468])\n",
      "\n",
      " lebel ( actual next character) is : 5\n",
      "\n",
      " probability assigned to the correct next char by the net is: 0.003941063769161701\n",
      "\n",
      " log likelihood is: -5.536304473876953\n",
      "\n",
      " negative log likelihood: 5.536304473876953\n",
      "===========================\n",
      "\n",
      " bigram example 2:e m (indeces 5, 13)\n",
      "\n",
      " input to the neural network is: 5\n",
      "\n",
      " output probabilities from  the neural network is: tensor([0.0065, 0.0179, 0.0056, 0.0117, 0.0447, 0.0065, 0.0021, 0.0075, 0.0022,\n",
      "        0.0068, 0.0158, 0.0051, 0.0026, 0.0041, 0.0024, 0.0071, 0.0065, 0.0010,\n",
      "        0.0206, 0.0048, 0.0109, 0.0067, 0.0113, 0.0006, 0.0026, 0.0005, 0.0106])\n",
      "\n",
      " lebel ( actual next character) is : 13\n",
      "\n",
      " probability assigned to the correct next char by the net is: 0.0040543111972510815\n",
      "\n",
      " log likelihood is: -5.507974624633789\n",
      "\n",
      " negative log likelihood: 5.507974624633789\n",
      "===========================\n",
      "\n",
      " bigram example 3:m m (indeces 13, 13)\n",
      "\n",
      " input to the neural network is: 13\n",
      "\n",
      " output probabilities from  the neural network is: tensor([0.0049, 0.0116, 0.0077, 0.0053, 0.0106, 0.0032, 0.0042, 0.0039, 0.0194,\n",
      "        0.0026, 0.0012, 0.0125, 0.0021, 0.0042, 0.0023, 0.0018, 0.0092, 0.0019,\n",
      "        0.0103, 0.0009, 0.0033, 0.0012, 0.0021, 0.0032, 0.0190, 0.0074, 0.0020])\n",
      "\n",
      " lebel ( actual next character) is : 13\n",
      "\n",
      " probability assigned to the correct next char by the net is: 0.004216378089040518\n",
      "\n",
      " log likelihood is: -5.468778610229492\n",
      "\n",
      " negative log likelihood: 5.468778610229492\n",
      "===========================\n",
      "\n",
      " bigram example 4:m a (indeces 13, 1)\n",
      "\n",
      " input to the neural network is: 13\n",
      "\n",
      " output probabilities from  the neural network is: tensor([0.0049, 0.0116, 0.0077, 0.0053, 0.0106, 0.0032, 0.0042, 0.0039, 0.0194,\n",
      "        0.0026, 0.0012, 0.0125, 0.0021, 0.0042, 0.0023, 0.0018, 0.0092, 0.0019,\n",
      "        0.0103, 0.0009, 0.0033, 0.0012, 0.0021, 0.0032, 0.0190, 0.0074, 0.0020])\n",
      "\n",
      " lebel ( actual next character) is : 1\n",
      "\n",
      " probability assigned to the correct next char by the net is: 0.011638503521680832\n",
      "\n",
      " log likelihood is: -4.453436374664307\n",
      "\n",
      " negative log likelihood: 4.453436374664307\n",
      "===========================\n",
      "\n",
      " bigram example 5:a . (indeces 1, 0)\n",
      "\n",
      " input to the neural network is: 1\n",
      "\n",
      " output probabilities from  the neural network is: tensor([0.0021, 0.0012, 0.0055, 0.0014, 0.0084, 0.0043, 0.0150, 0.0018, 0.0017,\n",
      "        0.0007, 0.0142, 0.0012, 0.0137, 0.0015, 0.0032, 0.0029, 0.0057, 0.0011,\n",
      "        0.0125, 0.0074, 0.0064, 0.0043, 0.0007, 0.0046, 0.0091, 0.0070, 0.0013])\n",
      "\n",
      " lebel ( actual next character) is : 0\n",
      "\n",
      " probability assigned to the correct next char by the net is: 0.002077222103253007\n",
      "\n",
      " log likelihood is: -6.176723957061768\n",
      "\n",
      " negative log likelihood: 6.176723957061768\n",
      "====================================\n",
      "\n",
      " the avarage log likelihood, i.e loss = 3.392902135848999\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(8)\n",
    "for i in range(5):\n",
    "    #i-th bigram\n",
    "    x =xs[i].item()\n",
    "    y = ys[i].item()\n",
    "    print('===========================')\n",
    "    print(f\"\\n bigram example {i + 1 }:{i_to_char[x]} {i_to_char[y]} (indeces {x}, {y})\")\n",
    "    print(f\"\\n input to the neural network is: {x}\")\n",
    "    print(f\"\\n output probabilities from  the neural network is: {probs[i]}\")\n",
    "    print(f\"\\n lebel ( actual next character) is : {y}\")\n",
    "    p = probs[i,y]\n",
    "    print(f\"\\n probability assigned to the correct next char by the net is: {p.item()}\")\n",
    "    logp = torch.log(p)\n",
    "    print(f\"\\n log likelihood is: {logp.item()}\")\n",
    "    nll = -logp\n",
    "    print(f\"\\n negative log likelihood: {nll.item()}\")\n",
    "    nlls[i] = nll\n",
    "print(\"====================================\")\n",
    "print(f\"\\n the avarage log likelihood, i.e loss = {nlls.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # using seed to ensure replicability\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0195, 0.0032, 0.0039, 0.0014, 0.0054, 0.0039, 0.0009, 0.0074, 0.0044,\n",
       "         0.0100, 0.0025, 0.0089, 0.0029, 0.0026, 0.0160, 0.0763, 0.0194, 0.0008,\n",
       "         0.0080, 0.0018, 0.0109, 0.0035, 0.0009, 0.0064, 0.0038, 0.0493, 0.0468],\n",
       "        [0.0065, 0.0179, 0.0056, 0.0117, 0.0447, 0.0065, 0.0021, 0.0075, 0.0022,\n",
       "         0.0068, 0.0158, 0.0051, 0.0026, 0.0041, 0.0024, 0.0071, 0.0065, 0.0010,\n",
       "         0.0206, 0.0048, 0.0109, 0.0067, 0.0113, 0.0006, 0.0026, 0.0005, 0.0106],\n",
       "        [0.0049, 0.0116, 0.0077, 0.0053, 0.0106, 0.0032, 0.0042, 0.0039, 0.0194,\n",
       "         0.0026, 0.0012, 0.0125, 0.0021, 0.0042, 0.0023, 0.0018, 0.0092, 0.0019,\n",
       "         0.0103, 0.0009, 0.0033, 0.0012, 0.0021, 0.0032, 0.0190, 0.0074, 0.0020],\n",
       "        [0.0049, 0.0116, 0.0077, 0.0053, 0.0106, 0.0032, 0.0042, 0.0039, 0.0194,\n",
       "         0.0026, 0.0012, 0.0125, 0.0021, 0.0042, 0.0023, 0.0018, 0.0092, 0.0019,\n",
       "         0.0103, 0.0009, 0.0033, 0.0012, 0.0021, 0.0032, 0.0190, 0.0074, 0.0020],\n",
       "        [0.0021, 0.0012, 0.0055, 0.0014, 0.0084, 0.0043, 0.0150, 0.0018, 0.0017,\n",
       "         0.0007, 0.0142, 0.0012, 0.0137, 0.0015, 0.0032, 0.0029, 0.0057, 0.0011,\n",
       "         0.0125, 0.0074, 0.0064, 0.0043, 0.0007, 0.0046, 0.0091, 0.0070, 0.0013]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "logits = (xenc @ W) # log-count\n",
    "counts = logits.exp() # equivalent of N in Bigrams\n",
    "probs = counts/ counts.sum() # probability for the next character ( the two consecutive lines are sofmax)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0039, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0041, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0042, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0116, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0021, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since it is one-hot encoded, we are interested in probabilities at the given index\n",
    "\n",
    "probs[0,5], probs[1, 13], probs[2, 13], probs[3, 1], probs[4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0039, 0.0041, 0.0042, 0.0116, 0.0021], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the probabilites with pytorch using the torch.arage fonction\n",
    "\n",
    "print(torch.arange(5))\n",
    "\n",
    "probs[torch.arange(5), ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4286, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -probs[torch.arange(5), ys].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.grad=None\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " shape is torch.Size([27, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0195,  0.0032,  0.0039,  0.0014,  0.0054, -0.1961,  0.0009,  0.0074,\n",
       "          0.0044,  0.0100,  0.0025,  0.0089,  0.0029,  0.0026,  0.0160,  0.0763,\n",
       "          0.0194,  0.0008,  0.0080,  0.0018,  0.0109,  0.0035,  0.0009,  0.0064,\n",
       "          0.0038,  0.0493,  0.0468],\n",
       "        [-0.1979,  0.0012,  0.0055,  0.0014,  0.0084,  0.0043,  0.0150,  0.0018,\n",
       "          0.0017,  0.0007,  0.0142,  0.0012,  0.0137,  0.0015,  0.0032,  0.0029,\n",
       "          0.0057,  0.0011,  0.0125,  0.0074,  0.0064,  0.0043,  0.0007,  0.0046,\n",
       "          0.0091,  0.0070,  0.0013],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0065,  0.0179,  0.0056,  0.0117,  0.0447,  0.0065,  0.0021,  0.0075,\n",
       "          0.0022,  0.0068,  0.0158,  0.0051,  0.0026, -0.1959,  0.0024,  0.0071,\n",
       "          0.0065,  0.0010,  0.0206,  0.0048,  0.0109,  0.0067,  0.0113,  0.0006,\n",
       "          0.0026,  0.0005,  0.0106],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0099, -0.1767,  0.0153,  0.0105,  0.0213,  0.0063,  0.0083,  0.0079,\n",
       "          0.0387,  0.0052,  0.0024,  0.0249,  0.0041, -0.1916,  0.0046,  0.0036,\n",
       "          0.0185,  0.0038,  0.0205,  0.0018,  0.0066,  0.0025,  0.0042,  0.0064,\n",
       "          0.0381,  0.0148,  0.0040],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\" shape is {W.grad.shape}\")\n",
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update the tensor\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 228146\n"
     ]
    }
   ],
   "source": [
    "# create a trainign set\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chrs = ['.'] + list (w) + ['.']\n",
    "    for ch1, ch2 in zip(chrs, chrs[1:]):\n",
    "        #print(f\"char1: {ch1} and char2: {ch2}\")\n",
    "        ix1 = char_to_i[ch1]\n",
    "        ix2 = char_to_i[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print(f\"number of examples: {num}\")\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.758953332901001\n",
      "3.3710989952087402\n",
      "3.1540420055389404\n",
      "3.020373582839966\n",
      "2.9277119636535645\n",
      "2.8604023456573486\n",
      "2.8097293376922607\n",
      "2.7701027393341064\n",
      "2.7380733489990234\n",
      "2.711496591567993\n",
      "2.6890034675598145\n",
      "2.6696884632110596\n",
      "2.6529300212860107\n",
      "2.638277292251587\n",
      "2.6253881454467773\n",
      "2.613990545272827\n",
      "2.603863477706909\n",
      "2.5948219299316406\n",
      "2.5867116451263428\n",
      "2.57940411567688\n",
      "2.572789192199707\n",
      "2.5667762756347656\n",
      "2.5612878799438477\n",
      "2.5562589168548584\n",
      "2.551633596420288\n",
      "2.547366142272949\n",
      "2.5434155464172363\n",
      "2.5397486686706543\n",
      "2.536336660385132\n",
      "2.533154249191284\n",
      "2.5301806926727295\n",
      "2.5273969173431396\n",
      "2.5247864723205566\n",
      "2.522334575653076\n",
      "2.520029067993164\n",
      "2.517857789993286\n",
      "2.515810966491699\n",
      "2.513878345489502\n",
      "2.512052059173584\n",
      "2.510324001312256\n",
      "2.5086867809295654\n",
      "2.5071346759796143\n",
      "2.5056614875793457\n",
      "2.5042612552642822\n",
      "2.502929210662842\n",
      "2.5016613006591797\n",
      "2.5004520416259766\n",
      "2.4992990493774414\n",
      "2.498197317123413\n",
      "2.497144937515259\n",
      "2.496137857437134\n",
      "2.495173692703247\n",
      "2.4942495822906494\n",
      "2.49336314201355\n",
      "2.4925124645233154\n",
      "2.4916954040527344\n",
      "2.4909095764160156\n",
      "2.4901540279388428\n",
      "2.4894261360168457\n",
      "2.488725185394287\n",
      "2.488049268722534\n",
      "2.4873976707458496\n",
      "2.4867680072784424\n",
      "2.4861602783203125\n",
      "2.4855728149414062\n",
      "2.4850049018859863\n",
      "2.484455108642578\n",
      "2.4839234352111816\n",
      "2.483407735824585\n",
      "2.4829084873199463\n",
      "2.482424259185791\n",
      "2.481955051422119\n",
      "2.481499195098877\n",
      "2.4810571670532227\n",
      "2.4806275367736816\n",
      "2.480210065841675\n",
      "2.479804515838623\n",
      "2.479410409927368\n",
      "2.4790267944335938\n",
      "2.4786536693573\n",
      "2.478290557861328\n",
      "2.4779367446899414\n",
      "2.477592706680298\n",
      "2.477257490158081\n",
      "2.4769303798675537\n",
      "2.476611852645874\n",
      "2.4763007164001465\n",
      "2.4759981632232666\n",
      "2.4757025241851807\n",
      "2.4754140377044678\n",
      "2.475132703781128\n",
      "2.4748575687408447\n",
      "2.4745898246765137\n",
      "2.474327802658081\n",
      "2.474071502685547\n",
      "2.4738216400146484\n",
      "2.4735772609710693\n",
      "2.4733383655548096\n",
      "2.473104476928711\n",
      "2.4728763103485107\n"
     ]
    }
   ],
   "source": [
    "# putting everything together now\n",
    "\n",
    "for k in range(100):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = (xenc @ W) # log-count\n",
    "    counts = logits.exp() # equivalent of N in Bigrams\n",
    "    probs = counts/ counts.sum(1, keepdim=True) # probability for the next character ( the two consecutive lines are sofmax)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    #backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The equivalent of smoothing in the gradient base learning\n",
    "# if all we initialize all the W to zero, W = torch.randn(), then logits will be zero and their expo will be all one, and the probabilities will be uniform. so avoid equal or zero\n",
    "# trying to incentifize W to be zero will achive better\n",
    "\n",
    "\n",
    "\n",
    "#Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 228146\n"
     ]
    }
   ],
   "source": [
    "# create a trainign set for regularized neuron\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chrs = ['.'] + list (w) + ['.']\n",
    "    for ch1, ch2 in zip(chrs, chrs[1:]):\n",
    "        #print(f\"char1: {ch1} and char2: {ch2}\")\n",
    "        ix1 = char_to_i[ch1]\n",
    "        ix2 = char_to_i[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print(f\"number of examples: {num}\")\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.768618583679199\n",
      "3.378804922103882\n",
      "3.1610894203186035\n",
      "3.027186155319214\n",
      "2.9344849586486816\n",
      "2.867231607437134\n",
      "2.816654920578003\n",
      "2.777147054672241\n",
      "2.7452545166015625\n",
      "2.7188308238983154\n",
      "2.6965057849884033\n",
      "2.6773719787597656\n",
      "2.6608054637908936\n",
      "2.6463515758514404\n",
      "2.633664846420288\n",
      "2.6224710941314697\n",
      "2.6125476360321045\n",
      "2.6037065982818604\n",
      "2.595794439315796\n",
      "2.5886807441711426\n",
      "2.5822560787200928\n",
      "2.5764291286468506\n",
      "2.5711233615875244\n",
      "2.566272735595703\n",
      "2.5618226528167725\n",
      "2.5577261447906494\n",
      "2.5539438724517822\n",
      "2.550442695617676\n",
      "2.547192335128784\n",
      "2.5441696643829346\n",
      "2.5413522720336914\n",
      "2.538722038269043\n",
      "2.536261796951294\n",
      "2.5339579582214355\n",
      "2.531797409057617\n",
      "2.5297679901123047\n",
      "2.527860164642334\n",
      "2.5260636806488037\n",
      "2.5243701934814453\n",
      "2.522773027420044\n",
      "2.52126407623291\n",
      "2.519836664199829\n",
      "2.5184857845306396\n",
      "2.517205238342285\n",
      "2.515990734100342\n",
      "2.5148370265960693\n",
      "2.5137407779693604\n",
      "2.51269793510437\n",
      "2.511704921722412\n",
      "2.5107579231262207\n",
      "2.509855031967163\n",
      "2.5089924335479736\n",
      "2.5081679821014404\n",
      "2.507380247116089\n",
      "2.5066258907318115\n",
      "2.5059030055999756\n",
      "2.5052108764648438\n",
      "2.5045459270477295\n",
      "2.5039076805114746\n",
      "2.503295660018921\n",
      "2.5027060508728027\n",
      "2.5021398067474365\n",
      "2.501594305038452\n",
      "2.5010693073272705\n",
      "2.500562906265259\n",
      "2.500075340270996\n",
      "2.4996049404144287\n",
      "2.499150514602661\n",
      "2.4987120628356934\n",
      "2.49828839302063\n",
      "2.4978787899017334\n",
      "2.4974827766418457\n",
      "2.4970998764038086\n",
      "2.4967293739318848\n",
      "2.496370315551758\n",
      "2.4960227012634277\n",
      "2.4956860542297363\n",
      "2.4953596591949463\n",
      "2.4950435161590576\n",
      "2.494736433029175\n",
      "2.494438648223877\n",
      "2.494149684906006\n",
      "2.4938690662384033\n",
      "2.4935967922210693\n",
      "2.4933321475982666\n",
      "2.493075132369995\n",
      "2.4928252696990967\n",
      "2.492582321166992\n",
      "2.4923462867736816\n",
      "2.492116689682007\n",
      "2.4918930530548096\n",
      "2.491675853729248\n",
      "2.491464614868164\n",
      "2.491258382797241\n",
      "2.491058111190796\n",
      "2.4908626079559326\n",
      "2.4906723499298096\n",
      "2.4904870986938477\n",
      "2.4903063774108887\n",
      "2.4901304244995117\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# putting everything together now\n",
    "\n",
    "for k in range(100):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = (xenc @ W) # log-count\n",
    "    counts = logits.exp() # equivalent of N in Bigrams\n",
    "    probs = counts/ counts.sum(1, keepdim=True) # probability for the next character ( the two consecutive lines are sofmax)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W**2).mean() # with regulazation layer\n",
    "    print(loss.item())\n",
    "\n",
    "    #backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cfay.\n",
      "a.\n",
      "nn.\n",
      "kohin.\n",
      "tolian.\n",
      "juwe.\n",
      "kilanaauranilevias.\n",
      "dedainrwieta.\n",
      "ssonielylarte.\n",
      "faveumerifontume.\n",
      "phynslenaruani.\n",
      "core.\n",
      "yaenon.\n",
      "ka.\n",
      "jabi.\n",
      "werimikimaynin.\n",
      "anaasn.\n"
     ]
    }
   ],
   "source": [
    "# sampling from neural net (borrowing code fro)\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "    out = []\n",
    "    index = 0\n",
    "    while True:\n",
    "        # probs = p[index]  # don't overwrite `p`\n",
    "        # index = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        # out.append(i_to_char[index])\n",
    "\n",
    "        #now \n",
    "        xenc = F.one_hot(torch.tensor([index]), num_classes=27).float()\n",
    "        logits = (xenc @ W) # log-count\n",
    "        counts = logits.exp() # equivalent of N in Bigrams\n",
    "        p = counts/ counts.sum(1, keepdim=True) # probability for the next character ( the two consecutive lines are sofmax)\n",
    "\n",
    "        index = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(i_to_char[index])\n",
    "        \n",
    "        if index == 0:\n",
    "            break\n",
    "    print(\"\".join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The output is the same as the bigram that we used becuase this is the identical model just with different\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
