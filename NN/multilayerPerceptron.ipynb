{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers_dim, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initializes the neural network parameters.\n",
    "        :param layers_dim: List containing the number of neurons in each layer.\n",
    "        :param learning_rate: Learning rate for gradient descent.\n",
    "        \"\"\"\n",
    "        self.param = {}  # Stores weights & biases\n",
    "        self.cach = {}  # Stores intermediate values (A, Z) for backprop\n",
    "        self.grads = {}  # Stores computed gradients\n",
    "        self.layers = len(layers_dim)  # Total number of layers\n",
    "        self.learning_rate = learning_rate  # Learning rate for updates\n",
    "        \n",
    "        # Initialize weights & biases\n",
    "        for l in range(1, self.layers):\n",
    "            self.param[f\"w{l}\"] = np.random.randn(layers_dim[l], layers_dim[l - 1]) * 0.01\n",
    "            self.param[f\"b{l}\"] = np.zeros((layers_dim[l], 1))\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def sigmoid_derivative(self, A):\n",
    "        return A * (1 - A)  # Derivative of Sigmoid: A * (1 - A)\n",
    "\n",
    "    def relu_derivative(self, Z):\n",
    "        return (Z > 0).astype(float)  # Derivative of ReLU: 1 for Z > 0, else 0\n",
    "\n",
    "    def forward_propagation(self, features):\n",
    "        \"\"\"\n",
    "        Performs forward propagation and stores activations & weighted sums.\n",
    "        :param features: Input features (n_features, m_samples)\n",
    "        \"\"\"\n",
    "        self.cach[\"A0\"] = features  # Input layer activations\n",
    "\n",
    "        for l in range(1, self.layers):\n",
    "            w, b = self.param[f\"w{l}\"], self.param[f\"b{l}\"]\n",
    "            z = np.dot(w, self.cach[f\"A{l - 1}\"]) + b\n",
    "            self.cach[f\"Z{l}\"] = z\n",
    "\n",
    "            # Use ReLU for hidden layers, Sigmoid for output layer\n",
    "            if l < self.layers - 1:\n",
    "                self.cach[f\"A{l}\"] = self.relu(z)\n",
    "            else:\n",
    "                self.cach[f\"A{l}\"] = self.sigmoid(z)  # Output layer\n",
    "\n",
    "        return self.cach[f\"A{self.layers - 1}\"]  # Return final predictions\n",
    "\n",
    "    def compute_loss(self, Y, AL):\n",
    "        \"\"\"\n",
    "        Computes the binary cross-entropy loss.\n",
    "        :param Y: True labels (1, m_samples)\n",
    "        :param AL: Predicted output (1, m_samples)\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        loss = -np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL)) / m\n",
    "        return loss\n",
    "\n",
    "    def backpropagation(self, Y):\n",
    "        \"\"\"\n",
    "        Performs backpropagation to compute gradients for weights & biases.\n",
    "        :param Y: True labels (1, m_samples)\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]  # Number of samples\n",
    "        L = self.layers - 1  # Output layer index\n",
    "\n",
    "        # Compute gradient of loss w.r.t. output activation\n",
    "        dA = -(np.divide(Y, self.cach[f\"A{L}\"]) - np.divide(1 - Y, 1 - self.cach[f\"A{L}\"]))\n",
    "\n",
    "        # Loop backward through layers\n",
    "        for l in reversed(range(1, self.layers)):\n",
    "            dZ = dA * (self.sigmoid_derivative(self.cach[f\"A{l}\"]) if l == L else self.relu_derivative(self.cach[f\"Z{l}\"]))\n",
    "            self.grads[f\"dW{l}\"] = np.dot(dZ, self.cach[f\"A{l-1}\"].T) / m\n",
    "            self.grads[f\"db{l}\"] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "            dA = np.dot(self.param[f\"w{l}\"].T, dZ)  # Backprop to previous layer\n",
    "\n",
    "    def update_parameters(self):\n",
    "        \"\"\"\n",
    "        Updates weights & biases using gradient descent.\n",
    "        \"\"\"\n",
    "        for l in range(1, self.layers):\n",
    "            self.param[f\"w{l}\"] -= self.learning_rate * self.grads[f\"dW{l}\"]\n",
    "            self.param[f\"b{l}\"] -= self.learning_rate * self.grads[f\"db{l}\"]\n",
    "\n",
    "    def train(self, X, Y, epochs=1000):\n",
    "        \"\"\"\n",
    "        Trains the neural network using forward propagation, loss computation, \n",
    "        backpropagation, and parameter updates.\n",
    "        :param X: Input features (n_features, m_samples)\n",
    "        :param Y: True labels (1, m_samples)\n",
    "        :param epochs: Number of training iterations\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            AL = self.forward_propagation(X)  # Forward pass\n",
    "            loss = self.compute_loss(Y, AL)  # Compute loss\n",
    "            self.backpropagation(Y)  # Backpropagation\n",
    "            self.update_parameters()  # Update weights\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Makes predictions using the trained model.\n",
    "        :param X: Input features (n_features, m_samples)\n",
    "        \"\"\"\n",
    "        AL = self.forward_propagation(X)\n",
    "        return (AL > 0.5).astype(int)  # Convert probabilities to binary output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
